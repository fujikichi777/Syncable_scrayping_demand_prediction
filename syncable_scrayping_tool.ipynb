{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# スクレイピング対象の URL にリクエストを送り HTML を取得する\n",
    "res = requests.get('https://syncable.biz/campaign')\n",
    "html_str = res.text\n",
    "\n",
    "# HTML を解析する\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "# タイトルを取得する\n",
    "title = soup.select(\"title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_title_completed=[]\n",
    "A=\"https://syncable.biz\"\n",
    "for i in url_title2:\n",
    "    url_title_completed.append(A+i.split('\"')[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options=Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "\n",
    "url=\"https://syncable.biz/explore/associate/social-challenge\"\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "dantai_label=[]\n",
    "\n",
    "for i in soup.find_all(\"h3\",\"chakra-text css-1emjuaw\"):\n",
    "    dantai_label.append(i.text)]\n",
    "url_label=[]\n",
    "for i in soup.find_all(\"a\",\"chakra-linkbox__overlay css-1s3d028\"):\n",
    "    url_label.append(A+str(i).split('\"')[3])\n",
    "\n",
    "\n",
    "options=Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "j=0\n",
    "dantaimei=[]\n",
    "dantaimei_label=[]\n",
    "for url in url_label:\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "    for i in soup.find_all(\"h2\",\"chakra-text css-o18mk2\"):\n",
    "        dantaimei.append(i.text)\n",
    "        dantaimei_label.append(dantai_label[j])\n",
    "    \n",
    "    j=j+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dantai_label=[]\n",
    "\n",
    "for i in soup.find_all(\"h3\",\"chakra-text css-1emjuaw\"):\n",
    "    dantai_label.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_label=[]\n",
    "for i in soup.find_all(\"a\",\"chakra-linkbox__overlay css-1s3d028\"):\n",
    "    url_label.append(A+str(i).split('\"')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43murl_label\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_label' is not defined"
     ]
    }
   ],
   "source": [
    "len(url_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /Users/fujiyoshitooru/Library/Python/3.8/lib/python/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/fujiyoshitooru/Library/Python/3.8/lib/python/site-packages (from selenium) (1.26.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chromedriver-binary-auto\n",
      "  Downloading chromedriver-binary-auto-0.3.1.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: chromedriver-binary-auto\n",
      "  Building wheel for chromedriver-binary-auto (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chromedriver-binary-auto: filename=chromedriver_binary_auto-0.3.1-py3-none-any.whl size=9504152 sha256=e7069a0f5ef2a46c8a4fec974956fa99170f465397b8e03215fef3077ded9c07\n",
      "  Stored in directory: /Users/fujiyoshitooru/Library/Caches/pip/wheels/1f/2d/78/1b53f8f3caa66a6418f96e1d510d7f2166bb2342ebd51ee65e\n",
      "Successfully built chromedriver-binary-auto\n",
      "Installing collected packages: chromedriver-binary-auto\n",
      "Successfully installed chromedriver-binary-auto-0.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromedriver-binary-auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import chromedriver_binary \n",
    "\n",
    "url_title_completed=[]\n",
    "A=\"https://syncable.biz\"\n",
    "for i in url_title2:\n",
    "    url_title_completed.append(A+i.split('\"')[3])\n",
    "\n",
    "options=Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "\n",
    "\n",
    "# 仮想ブラウザ起動、URL先のサイトにアクセス(chromedriverの保存先を指定 windows環境)\n",
    "\n",
    "# アクセスするURL(自分のQiitaの記事)\n",
    "setsumeibunn=[]\n",
    "jyuusho=[]\n",
    "kaishibi=[]\n",
    "shuuryoubi=[]\n",
    "danntai=[]\n",
    "for i in url_title_completed:\n",
    "# i=url_title_completed[0]\n",
    "    url = i\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "    div_tags=soup.find_all(\"div\", class_=\"__html-renderer-container css-0\")\n",
    "    div_texts = [tag.text for tag in div_tags]\n",
    "    if len(div_texts)>=1:\n",
    "        setsumeibunn.append(div_texts[0])\n",
    "    else:\n",
    "        setsumeibunn.append(\"nan\")\n",
    "    if len(soup.select(\".css-w4qtog\"))>=2:\n",
    "        jyuusho.append(soup.select(\".css-w4qtog\")[-2].text)\n",
    "    else:\n",
    "        jyuusho.append(\"nan\")\n",
    "    if len(soup.select(\".css-118bsz2\"))>=2:\n",
    "        kaishibi.append(soup.select(\".css-118bsz2\")[0].text)\n",
    "        shuuryoubi.append(soup.select(\".css-118bsz2\")[1].text)\n",
    "    else:\n",
    "        kaishibi.append(\"nan\")\n",
    "        shuuryoubi.append(\"nan\")\n",
    "    if len(soup.find_all(\"p\",\"chakra-text css-1igd86h\"))>=1:\n",
    "        danntai.append(soup.find_all(\"p\",\"chakra-text css-1igd86h\")[0].text)\n",
    "    else:\n",
    "        danntai.append(\"nan\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danntai_label=[]\n",
    "for i in danntai:\n",
    "    for j in dantaimei:\n",
    "        if i == j:\n",
    "            danntai_label.append(dantai_label[dantaimei.index(j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_texts = [tag.text for tag in div_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# スクレイピング対象の URL にリクエストを送り HTML を取得する\n",
    "res = requests.get('https://syncable.biz/campaign')\n",
    "html_str = res.text\n",
    "\n",
    "# HTML を解析する\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "# タイトルを取得する\n",
    "title = soup.select(\"title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options=Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "\n",
    "urls=[]\n",
    "for i in range(95):\n",
    "    urls.append(f\"https://syncable.biz/campaign?page={i+1}&rowsPerPage=20&sortBy=-created_at\")\n",
    "\n",
    "title=[]\n",
    "targetamount=[]\n",
    "amountnow=[]\n",
    "url_title=[]\n",
    "# 仮想ブラウザ起動、URL先のサイトにアクセス(chromedriverの保存先を指定 windows環境)\n",
    "\n",
    "# アクセスするURL(自分のQiitaの記事)\n",
    "for i in urls:\n",
    "    url = i\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "    title=title+soup.select(\".css-1pqcluv\")\n",
    "    amountnow=amountnow+soup.select(\".css-10fiots\")\n",
    "    targetamount=targetamount+soup.select(\".css-12a7mot\")\n",
    "    url_title=url_title+soup.select(\".css-1s3d028\")\n",
    "\n",
    "title2=[]\n",
    "amountnow2=[]\n",
    "targetamount2=[]\n",
    "url_title2=[]\n",
    "\n",
    "for i in title:\n",
    "    title2.append(i.text)\n",
    "for i in amountnow:\n",
    "    amountnow2.append(i.text)\n",
    "for i in targetamount:\n",
    "    targetamount2.append(i.text)\n",
    "for i in url_title:\n",
    "    url_title2.append(str(i))\n",
    "\n",
    "j=0\n",
    "for i in targetamount2:\n",
    "    if (\"円\" not in i) and (\"人\" not in i):\n",
    "        j=j+1\n",
    "\n",
    "for k in range(j):\n",
    "    targetamount2.remove(i)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in targetamount2:\n",
    "    if (\"円\" not in i) and (\"人\" not in i):\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(j):\n",
    "    targetamount2.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while pickling an object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/PDINe/Downloads/title.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while pickling an object"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('C:/Users/PDINe/Downloads/title.txt', 'wb')\n",
    "pickle.dump(title2, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in url_title:\n",
    "    url_title2.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/amountnow.txt', 'wb')\n",
    "pickle.dump(amountnow2, f)\n",
    "f.close()\n",
    "\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/targetamount.txt', 'wb')\n",
    "pickle.dump(targetamount2, f)\n",
    "f.close()\n",
    "\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/title.txt', 'wb')\n",
    "pickle.dump(title2, f)\n",
    "f.close()\n",
    "\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/url_title.txt', 'wb')\n",
    "pickle.dump(url_title2, f)\n",
    "f.close()\n",
    "\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/dantaimei.txt', 'wb')\n",
    "pickle.dump(dantaimei, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/dantaimei_label.txt', 'wb')\n",
    "pickle.dump(dantaimei_label, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/dantai_label.txt', 'wb')\n",
    "pickle.dump(dantai_label, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/setsumeibunn.txt', 'wb')\n",
    "pickle.dump(setsumeibunn, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/jyuusho.txt', 'wb')\n",
    "pickle.dump(jyuusho, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/kaishibi.txt', 'wb')\n",
    "pickle.dump(kaishibi, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/shuuryoubi.txt', 'wb')\n",
    "pickle.dump(shuuryoubi, f)\n",
    "f.close()\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/danntai.txt', 'wb')\n",
    "pickle.dump(danntai, f)\n",
    "f.close()\n",
    "# f = open('/Users/fujiyoshitooru/mdmg4/dump/danntai_label.txt', 'wb')\n",
    "# pickle.dump(danntai_label, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m title2 \u001b[38;5;241m=\u001b[39m \u001b[43municode\u001b[49m(title\u001b[38;5;241m.\u001b[39mstring)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": [
    "danntai_label=[]\n",
    "for i in danntai:\n",
    "    for j in dantaimei \n",
    "        if i == j:\n",
    "            danntai_label.append(dantai_label[dantaimei.index(j)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/targetamount.txt', 'rb')\n",
    "targetamount2 = pickle.load(f)\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/amountnow.txt', 'rb')\n",
    "amountnow2 = pickle.load(f)\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/title.txt', 'rb')\n",
    "title2 = pickle.load(f)\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/url_title.txt', 'rb')\n",
    "url_title2 = pickle.load(f)\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/dantaimei.txt', 'rb')\n",
    "dantaimei=pickle.load(f)\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/dantaimei_label.txt', 'rb')\n",
    "dantaimei_label=pickle.load(f)\n",
    "f = open('/Users/fujiyoshitooru/mdmg4/dump/dantai_label.txt', 'rb')\n",
    "dantai_label=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for i in range(95):\n",
    "    urls.append(f\"https://syncable.biz/campaign?page={i+1}&rowsPerPage=20&sortBy=-created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\pdine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\pdine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\pdine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from selenium) (4.12.1)\n",
      "Collecting websocket-client>=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\pdine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/9.4 MB 15.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.0/9.4 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.5/9.4 MB 24.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.9/9.4 MB 26.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.4/9.4 MB 27.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 28.7 MB/s eta 0:00:00\n",
      "Downloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
      "   ---------------------------------------- 0.0/475.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 475.7/475.7 kB 29.1 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB ? eta 0:00:00\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.2.0 cffi-1.16.0 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.22.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.26.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# aタグのhref属性を持つ要素をリスト化\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m elements \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements_by_xpath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//a[@href]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 各リンクの先のページを取得\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_xpath'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
